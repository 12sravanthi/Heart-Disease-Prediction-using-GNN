# -*- coding: utf-8 -*-
"""Major.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I8Pqs1afzGYIEMn_tlB_vrfDeZdHiJpv
"""

from google.colab import files


uploaded = files.upload()



import pandas as pd

data=pd.read_csv("Heart Disease.csv")

data

missing=data.isnull().sum()

missing

data.describe()

import seaborn as sns
import matplotlib.pyplot as plt

num_col=data.select_dtypes(include=["int","float"]).columns

for column in num_col:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[column], kde=True)
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Compute pairwise correlations between numerical features
correlation_matrix = data.corr()

# Plot correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Pairwise Correlation Heatmap of Numerical Features')
plt.show()

target_column_name = 'target'

# Plot bar plots for categorical features vs. target variable
for column in num_col:
    plt.figure(figsize=(8, 6))
    sns.barplot(x=column, y=target_column_name, data=data)
    plt.title(f'{column} vs. {target_column_name}')
    plt.xlabel(column)
    plt.ylabel(target_column_name)
    plt.show()

import networkx as nx

# Create an empty graph
G = nx.Graph()
# Add nodes to the graph, with attributes based on dataset columns
for idx, row in data.iterrows():
    # Add node with unique identifier (index) and attributes (features)
    G.add_node(idx, **row)

import numpy as np

# Define a function to calculate Euclidean distance between two instances
def euclidean_distance(instance1, instance2):
    # Convert instances to NumPy arrays for easy computation
    instance1_array = np.array(instance1)
    instance2_array = np.array(instance2)

    # Calculate Euclidean distance
    distance = np.linalg.norm(instance1_array - instance2_array)
    return distance

# Define a similarity threshold (you can adjust this based on your requirements)
similarity_threshold = 5  # For example

# Define relationships between nodes and add edges to the graph
for i in range(len(data)):
    for j in range(i+1, len(data)):
        # Calculate similarity between instances based on attributes
        similarity = euclidean_distance(data.iloc[i], data.iloc[j])

        # Check if similarity is below the threshold
        if similarity < similarity_threshold:
            G.add_edge(i, j)

import numpy as np
import networkx as nx
from scipy.sparse import csr_matrix

# Convert the graph into adjacency matrix
adj_matrix = nx.adjacency_matrix(G)

# Convert DataFrame into node feature matrix
# Assume 'features' contains the list of features/columns in your dataset
features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']
node_features = data[features].values

# Standardize node features (optional)
# You may choose to normalize or standardize the features based on your requirements
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
node_features_scaled = scaler.fit_transform(node_features)

# Convert the node feature matrix to a sparse matrix (optional)
# Sparse matrices can be more memory-efficient, especially for large datasets
node_features_sparse = csr_matrix(node_features_scaled)

import matplotlib.pyplot as plt

# Visualize the graph with adjusted parameters
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G, k=0.3, iterations=50)  # Adjust layout parameters
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=300, edge_color='gray', linewidths=0.5)
plt.title("Graph Visualization")
plt.show()

!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html
!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html
!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html
!pip install torch-geometric

import pandas as pd
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import networkx as nx

# Assuming your CSV has 13 columns as features and one column as the target variable
features = data.iloc[:, :-1].values.astype(float)
labels = data.iloc[:, -1].values.astype(int)

# Normalize input features
scaler = StandardScaler()
features = scaler.fit_transform(features)

# Split data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# Convert the data into PyTorch tensors
x_train = torch.tensor(x_train, dtype=torch.float)
x_test = torch.tensor(x_test, dtype=torch.float)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

# Construct a graph from features (using a simple K-nearest neighbors approach)
# You may need to adjust the parameters of the K-nearest neighbors algorithm based on your data
G = nx.karate_club_graph()
edge_index = torch.tensor(list(G.edges)).t().contiguous()

# Define a PyTorch Geometric Data object
graph_data = Data(x=x_train, edge_index=edge_index, y=y_train)

# Define the Graph Neural Network model
class GCN(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 64)  # Increase hidden units
        self.conv2 = GCNConv(64, num_classes)   # Adjust to match num_classes

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)  # Increase dropout
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]  # Number of features
num_classes = len(torch.unique(y_train))  # Number of classes
model = GCN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Decrease learning rate, add L2 regularization
criterion = torch.nn.CrossEntropyLoss()

# Train the model
def train():
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    return loss

# Test the model
def test():
    model.eval()
    logits = model(graph_data.x, graph_data.edge_index)
    pred = logits.argmax(dim=1)
    correct = pred.eq(graph_data.y).sum().item()
    return correct / len(graph_data.y)

# Main training loop
for epoch in range(300):  # Increase number of epochs
    loss = train()
    acc = test()
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
with torch.no_grad():
    model.eval()
    logits = model(x_test, edge_index)
    pred = logits.argmax(dim=1)
    test_acc = pred.eq(y_test).sum().item() / y_test.size(0)
    print(f'Test Accuracy: {test_acc:.4f}')

# Define the Graph Neural Network model
class GCN(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 128)
        self.conv2 = GCNConv(128, 64)
        self.conv3 = GCNConv(64, num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv3(x, edge_index)
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GCN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Adjusted learning rate and weight decay
criterion = torch.nn.CrossEntropyLoss()

# Train the model
def train():
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    return loss

# Test the model
def test():
    model.eval()
    logits = model(graph_data.x, graph_data.edge_index)
    pred = logits.argmax(dim=1)
    correct = pred.eq(graph_data.y).sum().item()
    return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train()
    acc = test()
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
with torch.no_grad():
    model.eval()
    logits = model(x_test, edge_index)
    pred = logits.argmax(dim=1)
    test_acc = pred.eq(y_test).sum().item() / y_test.size(0)
    print(f'Test Accuracy: {test_acc:.4f}')

from sklearn.metrics import confusion_matrix

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, pred)
print("Confusion Matrix:")
print(conf_matrix)

# Confusion matrix
conf_matrix = np.array([[74, 28],
                        [6, 97]])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", annot_kws={"size": 16})
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

# Calculate metrics
TP = conf_matrix[1, 1]
FP = conf_matrix[0, 1]
TN = conf_matrix[0, 0]
FN = conf_matrix[1, 0]

accuracy = (TP + TN) / np.sum(conf_matrix)
precision = TP / (TP + FP)
recall = TP / (TP + FN)
specificity = TN / (TN + FP)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f"Accuracy: {test_acc:.4f} ")
print(f"Precision: {precision:.4f}")
print(f"Recall (Sensitivity): {recall:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"F1 Score: {f1_score:.4f}")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv

# Define the Graph Attention Network model
class GAT(torch.nn.Module):
    def __init__(self, num_features, num_classes, num_heads=8):
        super(GAT, self).__init__()
        self.conv1 = GATConv(num_features, 64, heads=num_heads)
        self.conv2 = GATConv(64 * num_heads, num_classes, heads=1)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GAT(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Adjusted learning rate and weight decay
criterion = torch.nn.CrossEntropyLoss()

# Train the model
def train_gat(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x,graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_gat(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x,graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_gat(model, graph_data)
    acc = test_gat(model, graph_data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc = test_gat(model,y_test)
print(f'Test Accuracy: {test_acc:.4f}')
print(f"Precision: {precision:.4f}")
print(f"Recall (Sensitivity): {recall:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"F1 Score: {f1_score:.4f}")

from sklearn.metrics import confusion_matrix

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Get predictions on the test set
with torch.no_grad():
    model.eval()
    logits = model(x_test, edge_index)
    predictions = logits.argmax(dim=1)

# Convert PyTorch tensors to numpy arrays
y_true = y_test.cpu().numpy()
y_pred = predictions.cpu().numpy()

# Define class labels (if applicable)
# Replace this with your actual class labels
classes = ['Class 0', 'Class 1']

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get predictions on the test set
with torch.no_grad():
    model.eval()
    logits = model(x_test, edge_index)
    predictions = logits.argmax(dim=1)

# Convert PyTorch tensors to numpy arrays
y_true = y_test.cpu().numpy()
y_pred = predictions.cpu().numpy()

# Compute accuracy
accuracy = accuracy_score(y_true, y_pred)

# Compute precision
precision = precision_score(y_true, y_pred)

# Compute recall (sensitivity)
recall = recall_score(y_true, y_pred)

# Compute F1 score
f1 = f1_score(y_true, y_pred)

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Compute specificity
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
# Plot confusion matrix
plot_confusion_matrix(y_true, y_pred, classes)
print(f'Accuracy: {test_acc:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall (Sensitivity): {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f'Specificity: {specificity:.4f}')
print('Confusion Matrix:')
print(cm)

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

# Define the GraphSAGE model
class GraphSAGE(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(num_features, 64)
        self.conv2 = SAGEConv(64, num_classes)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Instantiate the GraphSAGE model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GraphSAGE(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Adjusted learning rate and weight decay
criterion = torch.nn.CrossEntropyLoss()

# Train the model
def train_graphsage(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out,graph_data.y)
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_graphsage(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_graphsage(model, data)
    acc = test_graphsage(model, data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc = test_graphsage(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Get predictions on the test set
with torch.no_grad():
    model.eval()
    logits = model(x_test, edge_index)
    predictions = logits.argmax(dim=1)

# Convert PyTorch tensors to numpy arrays
y_true = y_test.cpu().numpy()
y_pred = predictions.cpu().numpy()

# Define class labels (if applicable)
# Replace this with your actual class labels
classes = ['Class 0', 'Class 1']
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Compute accuracy
accuracy = accuracy_score(y_true, y_pred)

# Compute precision
precision = precision_score(y_true, y_pred)

# Compute recall
recall = recall_score(y_true, y_pred)

# Compute F1 score
f1 = f1_score(y_true, y_pred)

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Compute specificity
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

print(f'Test Accuracy: {test_acc:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f'Specificity: {specificity:.4f}')
print('Confusion Matrix:')
print(cm)
# Plot confusion matrix
plot_confusion_matrix(y_true, y_pred, classes)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GINConv

class GIN(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_features, 64), nn.ReLU(), nn.Linear(64, 64)))
        self.conv2 = GINConv(nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, num_classes)))

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GIN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

# Train the model
def train_gin(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_gin(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_gin(model, data)
    acc = test_gin(model, data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc = test_gin(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GINConv
from torch_geometric.data import Data

class GIN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_features, 64), nn.ReLU(), nn.Linear(64, 64)))
        self.conv2 = GINConv(nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, num_classes)))
        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with probability 0.5

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.conv2(x, edge_index))
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GIN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Train the model
def train_gin(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_gin(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_gin(model, data)
    acc = test_gin(model, data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc = test_gin(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GINConv
from torch_geometric.data import Data

class GIN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_features, 64), nn.ReLU(), nn.Linear(64, 64)))
        self.conv2 = GINConv(nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, num_classes)))

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GIN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Define label noise percentage
label_noise_percentage = 0.2  # Example: 20% label noise

# Generate random indices for label noise
num_samples = len(y_train)
num_noise_samples = int(num_samples * label_noise_percentage)
noise_indices = torch.randperm(num_samples)[:num_noise_samples]

# Apply label noise to the training labels
y_train_noise = y_train.clone()
y_train_noise[noise_indices] = torch.randint(0, num_classes, (num_noise_samples,))

# Train the model
def train_gin(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, y_train_noise)  # Use noisy labels
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_gin(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y)

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_gin(model, data)
    acc = test_gin(model, data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc = test_gin(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GINConv
from torch_geometric.data import Data
from sklearn.metrics import confusion_matrix

class GIN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_features, 64), nn.ReLU(), nn.Linear(64, 64)))
        self.conv2 = GINConv(nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, num_classes)))

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        return F.log_softmax(x, dim=1)

# Instantiate the model
num_features = x_train.shape[1]
num_classes = len(torch.unique(y_train))
model = GIN(num_features, num_classes)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Define label noise percentage
label_noise_percentage = 0.2  # Example: 20% label noise

# Generate random indices for label noise
num_samples = len(y_train)
num_noise_samples = int(num_samples * label_noise_percentage)
noise_indices = torch.randperm(num_samples)[:num_noise_samples]

# Apply label noise to the training labels
y_train_noise = y_train.clone()
y_train_noise[noise_indices] = torch.randint(0, num_classes, (num_noise_samples,))

# Train the model
def train_gin(model, data):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, y_train_noise)  # Use noisy labels
    loss.backward()
    optimizer.step()
    return loss.item()

# Test the model
def test_gin(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)
        correct = pred.eq(graph_data.y).sum().item()
        return correct / len(graph_data.y), pred.cpu().numpy(), graph_data.y.cpu().numpy()

# Main training loop
for epoch in range(500):  # Increased number of epochs
    loss = train_gin(model, data)
    acc, pred, true = test_gin(model, data)
    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

# Test the model on the test set
test_acc, test_pred, test_true = test_gin(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

# Calculate confusion matrix
conf_matrix = confusion_matrix(test_true, test_pred)
print("Confusion Matrix:")
print(conf_matrix)

test_acc, test_pred, test_true = test_gin(model, y_test)
print(f'Test Accuracy: {test_acc:.4f}')

# Calculate confusion matrix
conf_matrix = confusion_matrix(test_true, test_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Calculate evaluation metrics
precision = precision_score(test_true, test_pred)
recall = recall_score(test_true, test_pred)
f1 = f1_score(test_true, test_pred)
accuracy = accuracy_score(test_true, test_pred)

# Calculate specificity (TN / (TN + FP))
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Print evaluation metrics
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Accuracy:", accuracy)
print("Specificity:", specificity)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()